{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Horse-or-Human-WithAugmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30GC2JEwL2xj"
      },
      "source": [
        "## Again start by downloading the neccessary data into the Colab Instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLbtB3uGKPX"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXZT2UsyIVe_",
        "outputId": "dd0549fb-fc6f-402b-b0c0-7596a40c8cb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-12 16:09:02--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M   133MB/s    in 1.1s    \n",
            "\n",
            "2021-03-12 16:09:04 (133 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2021-03-12 16:09:04--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 64.233.183.128, 64.233.191.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "/tmp/validation-hor 100%[===================>]  10.95M  32.9MB/s    in 0.3s    \n",
            "\n",
            "2021-03-12 16:09:04 (32.9 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_-A0q1QL2xk",
        "outputId": "812016e6-51e9-4ab5-e96c-72b50302e01a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\n",
        "train_horse_names = os.listdir('/tmp/horse-or-human/horses')\n",
        "print(train_horse_names[:10])\n",
        "train_human_names = os.listdir('/tmp/horse-or-human/humans')\n",
        "print(train_human_names[:10])\n",
        "validation_horse_hames = os.listdir('/tmp/validation-horse-or-human/horses')\n",
        "print(validation_horse_hames[:10])\n",
        "validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\n",
        "print(validation_human_names[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['horse12-8.png', 'horse08-6.png', 'horse49-8.png', 'horse10-5.png', 'horse30-7.png', 'horse01-1.png', 'horse04-8.png', 'horse18-9.png', 'horse24-3.png', 'horse31-3.png']\n",
            "['human08-17.png', 'human12-21.png', 'human12-02.png', 'human12-01.png', 'human09-03.png', 'human14-25.png', 'human05-03.png', 'human15-09.png', 'human16-13.png', 'human06-09.png']\n",
            "['horse4-232.png', 'horse2-218.png', 'horse6-153.png', 'horse3-055.png', 'horse4-102.png', 'horse6-198.png', 'horse3-171.png', 'horse5-550.png', 'horse1-241.png', 'horse4-345.png']\n",
            "['valhuman05-17.png', 'valhuman01-09.png', 'valhuman02-24.png', 'valhuman01-12.png', 'valhuman02-19.png', 'valhuman04-12.png', 'valhuman05-18.png', 'valhuman04-09.png', 'valhuman01-00.png', 'valhuman01-14.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DFi8qSIL2xl"
      },
      "source": [
        "## Then again define your model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 100x100 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKYbRjx8L2xl"
      },
      "source": [
        "## Now when we organize the data into Generators note how we use many more kinds of Data Augmentation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClebU9NJg99G",
        "outputId": "9781aab9-bd19-45ee-8fcd-f0d87ff1fd54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be augmented\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(100, 100),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human',\n",
        "        target_size=(100, 100),\n",
        "        class_mode='binary')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcGH0D3CL2xm"
      },
      "source": [
        "## Train your model with the new augmented data\n",
        "Since we now have more data due to the data augmentation this training process will take a bit longer than the last time. However, you'll find that the results are much better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb1_lgobv81m",
        "outputId": "c48751c9-0560-47df-aae5-2d8424cad7c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data=validation_generator)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 8s 823ms/step - loss: 0.6905 - acc: 0.4879 - val_loss: 0.6935 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.6792 - acc: 0.5300 - val_loss: 0.6722 - val_acc: 0.5820\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 7s 894ms/step - loss: 0.6719 - acc: 0.6764 - val_loss: 0.6524 - val_acc: 0.8164\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 7s 891ms/step - loss: 0.6515 - acc: 0.7021 - val_loss: 0.6585 - val_acc: 0.5156\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 0.6402 - acc: 0.6503 - val_loss: 0.5890 - val_acc: 0.7695\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 7s 906ms/step - loss: 0.6223 - acc: 0.6717 - val_loss: 0.6105 - val_acc: 0.6211\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.5624 - acc: 0.7646 - val_loss: 0.6873 - val_acc: 0.5195\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 8s 996ms/step - loss: 0.5736 - acc: 0.7223 - val_loss: 0.5560 - val_acc: 0.6914\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 7s 882ms/step - loss: 0.5348 - acc: 0.7578 - val_loss: 0.8233 - val_acc: 0.5234\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.5506 - acc: 0.7466 - val_loss: 0.4839 - val_acc: 0.7617\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 7s 895ms/step - loss: 0.5103 - acc: 0.7666 - val_loss: 0.4020 - val_acc: 0.8672\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 7s 900ms/step - loss: 0.6008 - acc: 0.6762 - val_loss: 1.1726 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 7s 878ms/step - loss: 0.5144 - acc: 0.7503 - val_loss: 0.6349 - val_acc: 0.6016\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 7s 879ms/step - loss: 0.5202 - acc: 0.7356 - val_loss: 1.3275 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 7s 892ms/step - loss: 0.5181 - acc: 0.7412 - val_loss: 0.7182 - val_acc: 0.5742\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.4490 - acc: 0.8063 - val_loss: 0.7918 - val_acc: 0.5508\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.5752 - acc: 0.6899 - val_loss: 0.8979 - val_acc: 0.5312\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 7s 875ms/step - loss: 0.4379 - acc: 0.8003 - val_loss: 0.9187 - val_acc: 0.5586\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 8s 989ms/step - loss: 0.4259 - acc: 0.8281 - val_loss: 1.1149 - val_acc: 0.5352\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 7s 911ms/step - loss: 0.4043 - acc: 0.8097 - val_loss: 1.3598 - val_acc: 0.5469\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 7s 897ms/step - loss: 0.3699 - acc: 0.8570 - val_loss: 0.8070 - val_acc: 0.7109\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.3830 - acc: 0.8121 - val_loss: 1.1589 - val_acc: 0.6328\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 7s 882ms/step - loss: 0.3295 - acc: 0.8536 - val_loss: 1.8999 - val_acc: 0.5430\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 7s 884ms/step - loss: 0.2909 - acc: 0.8822 - val_loss: 2.2079 - val_acc: 0.5469\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.2218 - acc: 0.9286 - val_loss: 2.4916 - val_acc: 0.5469\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 7s 886ms/step - loss: 0.2561 - acc: 0.8873 - val_loss: 1.3747 - val_acc: 0.6484\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 7s 900ms/step - loss: 0.2770 - acc: 0.8677 - val_loss: 1.7472 - val_acc: 0.6133\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 7s 878ms/step - loss: 0.2720 - acc: 0.8813 - val_loss: 1.5847 - val_acc: 0.6406\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.2458 - acc: 0.8947 - val_loss: 2.2089 - val_acc: 0.5547\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 7s 908ms/step - loss: 0.2434 - acc: 0.8951 - val_loss: 2.8684 - val_acc: 0.5312\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 7s 904ms/step - loss: 0.2014 - acc: 0.9299 - val_loss: 3.4454 - val_acc: 0.5156\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 8s 941ms/step - loss: 0.2169 - acc: 0.8998 - val_loss: 3.4241 - val_acc: 0.5234\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1983 - acc: 0.9299 - val_loss: 2.5795 - val_acc: 0.5586\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2065 - acc: 0.9110 - val_loss: 2.7609 - val_acc: 0.5625\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 8s 961ms/step - loss: 0.1495 - acc: 0.9452 - val_loss: 3.6536 - val_acc: 0.5430\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 8s 942ms/step - loss: 0.1454 - acc: 0.9482 - val_loss: 3.7045 - val_acc: 0.5469\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 8s 947ms/step - loss: 0.1643 - acc: 0.9232 - val_loss: 2.9242 - val_acc: 0.6055\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.1690 - acc: 0.9370 - val_loss: 2.9361 - val_acc: 0.5781\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 8s 976ms/step - loss: 0.1422 - acc: 0.9428 - val_loss: 4.2556 - val_acc: 0.5273\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.1607 - acc: 0.9388 - val_loss: 4.1329 - val_acc: 0.5352\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1224 - acc: 0.9617 - val_loss: 3.6061 - val_acc: 0.5469\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1200 - acc: 0.9602 - val_loss: 3.6594 - val_acc: 0.5547\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 7s 888ms/step - loss: 0.1250 - acc: 0.9499 - val_loss: 3.6632 - val_acc: 0.5664\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 7s 871ms/step - loss: 0.1265 - acc: 0.9541 - val_loss: 3.1932 - val_acc: 0.5977\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 7s 870ms/step - loss: 0.1261 - acc: 0.9498 - val_loss: 4.4381 - val_acc: 0.5352\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 7s 868ms/step - loss: 0.1083 - acc: 0.9622 - val_loss: 4.1392 - val_acc: 0.5547\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 7s 917ms/step - loss: 0.1034 - acc: 0.9635 - val_loss: 3.9206 - val_acc: 0.5664\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 8s 953ms/step - loss: 0.1005 - acc: 0.9540 - val_loss: 4.6980 - val_acc: 0.5391\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.1001 - acc: 0.9558 - val_loss: 4.4550 - val_acc: 0.5586\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 8s 929ms/step - loss: 0.0769 - acc: 0.9760 - val_loss: 4.0077 - val_acc: 0.5898\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 8s 953ms/step - loss: 0.0745 - acc: 0.9749 - val_loss: 4.3446 - val_acc: 0.5781\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 8s 968ms/step - loss: 0.0797 - acc: 0.9678 - val_loss: 3.4762 - val_acc: 0.6211\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1339 - acc: 0.9519 - val_loss: 2.6342 - val_acc: 0.6406\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 8s 971ms/step - loss: 0.0979 - acc: 0.9689 - val_loss: 2.5022 - val_acc: 0.6602\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 8s 928ms/step - loss: 0.0913 - acc: 0.9619 - val_loss: 3.5422 - val_acc: 0.6250\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 8s 936ms/step - loss: 0.0674 - acc: 0.9738 - val_loss: 4.2000 - val_acc: 0.5938\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0725 - acc: 0.9831 - val_loss: 4.7330 - val_acc: 0.5820\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 7s 897ms/step - loss: 0.0736 - acc: 0.9782 - val_loss: 4.3321 - val_acc: 0.5977\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0581 - acc: 0.9790 - val_loss: 3.9372 - val_acc: 0.6289\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 8s 936ms/step - loss: 0.0665 - acc: 0.9750 - val_loss: 3.2626 - val_acc: 0.6602\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 8s 935ms/step - loss: 0.0802 - acc: 0.9737 - val_loss: 7.0827 - val_acc: 0.5156\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 8s 938ms/step - loss: 0.5267 - acc: 0.8267 - val_loss: 2.2159 - val_acc: 0.6602\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 7s 902ms/step - loss: 0.1978 - acc: 0.9079 - val_loss: 1.7628 - val_acc: 0.6719\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 7s 880ms/step - loss: 0.1387 - acc: 0.9437 - val_loss: 3.4853 - val_acc: 0.5586\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 7s 872ms/step - loss: 0.1312 - acc: 0.9480 - val_loss: 3.4786 - val_acc: 0.5625\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 7s 879ms/step - loss: 0.1291 - acc: 0.9576 - val_loss: 2.7507 - val_acc: 0.6094\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 8s 993ms/step - loss: 0.0701 - acc: 0.9821 - val_loss: 3.3694 - val_acc: 0.5898\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.0635 - acc: 0.9816 - val_loss: 2.8273 - val_acc: 0.6484\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.0632 - acc: 0.9775 - val_loss: 3.7666 - val_acc: 0.6133\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 7s 874ms/step - loss: 0.0767 - acc: 0.9705 - val_loss: 3.6862 - val_acc: 0.6250\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 0.0601 - acc: 0.9819 - val_loss: 3.7306 - val_acc: 0.6250\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 7s 888ms/step - loss: 0.0711 - acc: 0.9748 - val_loss: 3.9035 - val_acc: 0.6133\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0506 - acc: 0.9794 - val_loss: 3.9263 - val_acc: 0.6250\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 0.0597 - acc: 0.9801 - val_loss: 4.0838 - val_acc: 0.6211\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 7s 914ms/step - loss: 0.0485 - acc: 0.9848 - val_loss: 3.4389 - val_acc: 0.6562\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0528 - acc: 0.9828 - val_loss: 4.3038 - val_acc: 0.6172\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 8s 946ms/step - loss: 0.0468 - acc: 0.9819 - val_loss: 4.5586 - val_acc: 0.6172\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.0370 - acc: 0.9892 - val_loss: 3.4857 - val_acc: 0.6641\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 8s 952ms/step - loss: 0.0603 - acc: 0.9769 - val_loss: 4.1469 - val_acc: 0.6289\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 8s 947ms/step - loss: 0.0516 - acc: 0.9831 - val_loss: 4.8929 - val_acc: 0.5859\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 7s 891ms/step - loss: 0.0906 - acc: 0.9729 - val_loss: 3.4647 - val_acc: 0.6562\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 8s 926ms/step - loss: 0.0569 - acc: 0.9802 - val_loss: 3.4201 - val_acc: 0.6562\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 7s 1s/step - loss: 0.0614 - acc: 0.9759 - val_loss: 4.3340 - val_acc: 0.6133\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 7s 882ms/step - loss: 0.0323 - acc: 0.9914 - val_loss: 3.4470 - val_acc: 0.6523\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.0569 - acc: 0.9787 - val_loss: 3.8656 - val_acc: 0.6484\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 7s 913ms/step - loss: 0.0460 - acc: 0.9894 - val_loss: 5.4372 - val_acc: 0.5938\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 8s 939ms/step - loss: 0.0681 - acc: 0.9707 - val_loss: 6.6852 - val_acc: 0.5469\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 0.1172 - acc: 0.9511 - val_loss: 2.9381 - val_acc: 0.6758\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 7s 882ms/step - loss: 0.0653 - acc: 0.9810 - val_loss: 3.8598 - val_acc: 0.6211\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 7s 879ms/step - loss: 0.0575 - acc: 0.9756 - val_loss: 4.3487 - val_acc: 0.6172\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 7s 885ms/step - loss: 0.0385 - acc: 0.9883 - val_loss: 3.4586 - val_acc: 0.6562\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 7s 875ms/step - loss: 0.0369 - acc: 0.9889 - val_loss: 4.3410 - val_acc: 0.6172\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 8s 997ms/step - loss: 0.0335 - acc: 0.9873 - val_loss: 4.0049 - val_acc: 0.6367\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 7s 887ms/step - loss: 0.0411 - acc: 0.9878 - val_loss: 4.2902 - val_acc: 0.6211\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 7s 888ms/step - loss: 0.0318 - acc: 0.9915 - val_loss: 4.4194 - val_acc: 0.6172\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 7s 877ms/step - loss: 0.0319 - acc: 0.9860 - val_loss: 3.5895 - val_acc: 0.6602\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 7s 881ms/step - loss: 0.0602 - acc: 0.9708 - val_loss: 5.7680 - val_acc: 0.5781\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 7s 875ms/step - loss: 0.0593 - acc: 0.9782 - val_loss: 6.6140 - val_acc: 0.5547\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 7s 876ms/step - loss: 0.0945 - acc: 0.9664 - val_loss: 3.9559 - val_acc: 0.6406\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 7s 865ms/step - loss: 0.0456 - acc: 0.9863 - val_loss: 3.4147 - val_acc: 0.6641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "## Try Running the Model Again\n",
        "\n",
        "Can you confuse it this time? Or did the extra data augmentation help the model generalize? What do you think it was about your confusing examples that are no longer confusing (or what is still confusing)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWp43WxJDNT",
        "outputId": "8512d047-83d7-4595-d15e-21b45a627c4f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(100, 100))\n",
        "  x = image.img_to_array(img)\n",
        "  x = x/255.0\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]<0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5258aa27-68ee-46e0-96b9-1259f03031d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5258aa27-68ee-46e0-96b9-1259f03031d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving horse-197199_640 (1).jpg to horse-197199_640 (1) (2).jpg\n",
            "[[0.9925291]]\n",
            "[0.9925291]\n",
            "horse-197199_640 (1).jpg is a horse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzVJDupFL2xm"
      },
      "source": [
        "## Finally again lets visualize some of the layers for intuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_9ZT7CL2xn"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    n_features = 5\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    #plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}